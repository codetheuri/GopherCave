A great name for this project that highlights its Go roots would be **GopherCave** or **Go-Archiver**. Since it creates a local "vault" of a website, **GopherCave** has a nice ring to it.

Below is a professional README.md file designed to showcase the project's architecture and capabilities.

**README.md**
-------------

Markdown

\# GopherCaveA high-performance, concurrent web archiver and scraper built in Go. \`GopherCave\` crawls a target domain, downloads all necessary assets (CSS, JS, Images, Icons), and rewrites internal links to create a fully functional offline mirror of the site.## üöÄ Features\* \*\*Recursive Crawling\*\*: Automatically follows internal links up to a specified depth.\* \*\*Concurrent Workers\*\*: Uses Go routines and a semaphore pattern to fetch multiple pages simultaneously without overwhelming the host.\* \*\*Asset Mirroring\*\*: Downloads styles, scripts, and images to a local \`assets/\` folder.\* \*\*Link Normalization\*\*: Rewrites HTML links to support absolute root-relative navigation.\* \*\*Built-in Preview Server\*\*: Includes a local HTTP server to view your archived sites instantly.\* \*\*Metadata Extraction\*\*: Saves structural components like headers, footers, and cards into structured JSON files.## üõ†Ô∏è Project Structure\* \`cmd/scraper\`: Entry point for the CLI application.\* \`internal/crawler\`: The core logic for recursion and concurrency management.\* \`internal/fetcher\`: Handles HTTP requests with custom User-Agents and timeouts.\* \`internal/parser\`: Uses \`goquery\` to extract links and rewrite HTML attributes.\* \`internal/saver\`: Manages the filesystem, directory nesting, and asset persistence.\* \`internal/server\`: A lightweight static file server for archived content.## üì• Installation1. Clone the repository: ¬† \`\`\`bash ¬† git clone \[https://github.com/YOUR\_USERNAME/GopherCave.git\](https://github.com/YOUR\_USERNAME/GopherCave.git) ¬† cd GopherCave

1.  Install dependencies:Bashgo mod tidygo get \[github.com/schollz/progressbar/v3\](https://github.com/schollz/progressbar/v3)go get \[github.com/PuerkitoBio/goquery\](https://github.com/PuerkitoBio/goquery)
    

**üñ•Ô∏è Usage**
-------------

Run the scraper by providing a target URL:

Bash

go run ./cmd/scraper \[https://example.com/dashboard/\](https://example.com/dashboard/)

Once the crawl is complete, the preview server will start automatically. Open your browser to:

http://localhost:8080

**‚öôÔ∏è Configuration**
--------------------

Current hardcoded limits (can be modified in internal/crawler/crawler.go):

*   **Max Depth**: 2 (Levels deep from the starting URL)
    
*   **Concurrency**: 5 (Simultaneous workers)
    
*   **Politeness Delay**: 100ms between requests
    

**üìù License**
--------------

MIT


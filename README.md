# GopherCave

**High-performance, concurrent website archiver and offline mirror creator written in Go.**

GopherCave crawls a target website, downloads HTML pages together with all referenced assets (CSS, JavaScript, images, favicons, etc.), rewrites internal links to work offline, and saves everything in a structured local directory â€” giving you a fully browsable static copy of the site.

[![Go Version](https://img.shields.io/badge/Go-1.21+-00ADD8?logo=go&logoColor=white)](https://go.dev)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## âœ¨ Features

- **Recursive crawling** â€” follows internal links up to a configurable depth
- **Concurrent fetching** â€” uses goroutines + semaphore to download multiple resources safely and efficiently
- **Asset mirroring** â€” saves stylesheets, scripts, images and other files into `/assets/`
- **Link rewriting** â€” converts absolute, root-relative and protocol-relative URLs so the offline copy navigates correctly
- **Built-in preview server** â€” instantly view your archived site in the browser
- **Metadata extraction** â€” saves structural elements (headers, footers, cards, etc.) as clean JSON
- **Polite crawling** â€” configurable delay between requests to avoid overwhelming servers

## ğŸ“‚ Project Layout

```text
GopherCave/
â”œâ”€â”€ cmd/
â”‚   â””â”€â”€ scraper/           # CLI entry point
â”œâ”€â”€ internal/
â”‚   â”œâ”€â”€ crawler/           # Recursion logic & concurrency control
â”‚   â”œâ”€â”€ fetcher/           # HTTP client with UA, timeouts, redirects
â”‚   â”œâ”€â”€ parser/            # goquery-based HTML parsing & link rewriting
â”‚   â”œâ”€â”€ saver/             # Filesystem layout, directory creation, asset storage
â”‚   â””â”€â”€ server/            # Simple static HTTP preview server
â”œâ”€â”€ go.mod
â””â”€â”€ README.md
```


## ğŸ“¥ Installation
# 1. Clone the repository
git clone https://github.com/codetheuri/GopherCave.git
cd GopherCave

## 2. Download dependencies

```text
go mod tidy
```

## ğŸš€ Quick Start
# Basic usage â€” crawl and preview
```
go run ./cmd/scraper https://example.com/blog/
```

# After crawling finishes, the preview server starts automatically.
# Open in browser: http://localhost:8080
## âš™ï¸ Configuration (for now)
All important limits are currently defined as constants in internal/crawler/crawler.go. Edit and recompile to change them:
```
const (
    MaxDepth         = 2
    MaxConcurrency   = 5
    PolitenessDelay  = 100 * time.Millisecond
    // ...
)
```
## ğŸ› ï¸ Example Output Structure
After running on https://example.com/:
 ```
 output/
â”œâ”€â”€ index.html
â”œâ”€â”€ blog/
â”‚   â”œâ”€â”€ post-1/
â”‚   â”‚   â””â”€â”€ index.html
â”‚   â””â”€â”€ post-2/
â”‚       â””â”€â”€ index.html
â””â”€â”€ assets/
    â”œâ”€â”€ css/
    â”‚   â””â”€â”€ style.min.css
    â”œâ”€â”€ js/
    â”‚   â””â”€â”€ main.bundle.js
    â”œâ”€â”€ images/
    â”‚   â”œâ”€â”€ logo.svg
    â”‚   â””â”€â”€ hero-01.jpg
    â””â”€â”€ favicon.ico
    ```

